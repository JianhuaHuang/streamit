---
author: "Jianhua Huang"
date: "July 23, 2016"
title: "Streamline Routine Modeling Work in R: streamlineR"
output:
  md_document:
    toc: true
    variant: markdown_github
---

# Streamline Routine Modeling Work in R: streamlineR
**For an interactive presentation version, please check [here](https://jianhua.shinyapps.io/streamlineR_shinyapp/)**


```{r,set.options, include=FALSE}
# dir <- 'F:/Projects/Rpackage/streamlineR'
dir <- 'C:/Users/Jianhua/Dropbox/work_doc/Rpackage/streamlineR'
knitr::opts_chunk$set(echo = TRUE, root.dir = dir, warning=FALSE, message=FALSE)
```


This package is designed to streamline the routine modeling work, especially for scoring. It provides some handy functions to bin numerical variables, replace numerical variables with Weight of Evidence (WOE), ranking variables by Information Values (IV), plotting the successful/failure rates, check model performance based on AUC, and so on.This package also provides the useful function to convert the model output (e.g., coefficients) to graph/tables that are easier to understand for non-technical audience.

The following example illustrates how to use the `streamlineR` package to prepare data, build models, and visualize results. 

## Packages Setup
This analysis relies on other packages. If these packages are not available yet in your computer, you need to install them with the following commands.

#### Install Dependent Packages 
```{r, eval=FALSE}
# If the default mirror is blocked, choose another mirror to install R packages, 
# chooseCRANmirror()
sapply(c('dplyr', 'car', 'caret', 'e1071', 'knitr', 'reshape2', 'corrplot','rpart', 
  'scales', 'shiny', 'survival', 'gridExtra', 'devtools', 'pec', 'MASS', 'pROC', 
  'manipulate'), 
  install.packages)
```

#### Load Dependent Packages 
After installing these packages, you need to load them into R, so that you can use the functions in those packages. 
```{r}
# Load pacakges
sapply(c('dplyr', 'car', 'caret', 'e1071', 'knitr', 'reshape2', 'corrplot','rpart', 
  'scales', 'survival', 'gridExtra', 'devtools', 'pec', 'MASS', 'pROC', 
  'manipulate'), 
  require, character.only = TRUE)
```

#### Install `streamlineR` from Github 
The `streamlineR` package is under development on the Github platform. The package can be installed using the `install_github` function from the `devtools` package. If the `install_github` function does not work, you can download the package from [here](https://api.github.com/repos/JianhuaHuang/streamlineR/zipball/master), and then install the package locally in Rstudio (Tools -> Install Packages -> Install from -> Package Archive File (.zip; .tar.gz))
```{r}
# If the install_github does not work, you can download the package from github,
# and then install the package locally: 
# https://api.github.com/repos/JianhuaHuang/streamlineR/zipball/master 
devtools::install_github('JianhuaHuang/streamlineR')
library(streamlineR)
```


## Data Preparation
In this example, I analyzed the primary biliary cirrhosis (PBC) data set from the survival package. The details of this data set is available [here](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/pbc.html), or you can run  `?survival::pbc` to find the data description within R. 

#### Load Data 
The data set can be loaded into R directly by calling the data from the package. Because the sample size is a little small, I increased the sample size by resampling the data 10000 times. 
```{r}
dt <- survival::pbc %>%
  transmute(age = round(age), gender = sex, platelet, stage = as.character(stage), 
    time, status = as.numeric(status %in% c(1, 2))) %>%
  filter(!is.na(stage))

set.seed(1111)  # reproducible
dt <- dt[sample(nrow(dt), 10000, replace = T), ]
row.names(dt) <- NULL
dim(dt)
str(dt)
head(dt)
```

#### Split Data into Training and Test Datasets 
Before doing any analysis, I held out some data as test data set. The `createDataPartition` function (from `caret` package) is used to split the data into training and test data sets. The training data set is used to develop the model, and the test data set is used to test the model performance. 
```{r}
set.seed(1111)
ind.train <- createDataPartition(dt$status, p = .7, list = FALSE)
dt.train <- dt[ind.train, ]
dt.test <- dt[-ind.train, ]
row.names(dt.train) <- NULL
row.names(dt.test) <- NULL
dim(dt.train)
dim(dt.test)

# preserve the original values
dt.train.bk <- dt.train
dt.test.bk <- dt.test
```


## Bin Training Data Based on Regression Coefficients: `bin.knn`
Before building the model with training data, we may need to convert some numerical variables to categorical variables by binning the numerical values into different groups, so that we can model the non-linear relationship between the independent and dependent variables. This package provides two methods to find the cut points for binning: `bin.knn` and `bin.rpart`. 

The `bin.knn` method finds the cut points based on the regression coefficients using the KNN algorithm. Generally, the  `bin.knn` finds the cut points through the following steps:
1. Divide the independent variable x into some small bucket (e.g., 20 bucket)
2. Build a univariate model using x and y
3. Get the regression coefficients for each bucket
4. Use the KNN algorithm to bin the buckets into bigger groups (e.g., 5 groups), based on their orders and regression coefficients.   

The `bin.knn` method can now deal with Generalized Linear Models (GLM) and Survival Model (class of coxph in R). The `bin.knn` function takes four arguments: formula, data, n.group, and min.bucket. The min.bucket is the minimum ratio of records in each bucket. The min.bucket should be small enough to make that there are enough buckets for binning, and big enough to guarantee that the estimated regression coefficients are statistically reliable for all buckets.

The following function bins the 20 buckets (including NA) into 5 groups represented by different colors. We can extract the cut points based on the colors for different groups. 
```{r}
bin.knn(status ~ platelet, data = dt.train, n.group = 5, min.bucket = 0.05)
```

Although the `bin.knn` function take into acount both the order and coefficient of each bucket, it may bin some buckets together if the coefficients for some neighboring buckets are very different. In this case, we may need to change the number of groups and/or buckets by adjusting the `n.group` and/or `min.bucket` argument. This can be done interactively using the manipulate function as follows:
```{r,eval=FALSE}
manipulate(bin.knn(status ~ platelet, data = dt.train, n.group, min.bucket),
  n.group = slider(1, 10, step = 1, initial = 5, label = 'Number of Groups'),
  min.bucket = slider(0.01, .2, step = 0.01, initial = 0.05,
    label = 'Minimum Population'))
```
By changing the `n.group` and `min.bucket` repeatedly, we may be able to find the appropriate cut points. 


## Bin Training Data Based on rpart: `bin.rpart` 
Although the `bin.knn` method is intuitive and provides the visualization, it is difficult to find the optimal cut points manually. Another binning method `bin.rpart`, can help us to find the optimal cut points automatically, and can be applied to more models. 

#### Decision Tree Algorithm (Recursive Partitioning): `rpart`
The `bin.rpart` relies on the output from `rpart` (recursive partitioning), a famous algorithm used to build the decision tree. The `rpart` function can produce the optimal splits of numerical data, and generates a tree-structure nodes. Based on these splits and nodes, we can extract the cut points and bin the numerical values into different groups. 
```{r}
rpart(formula = status ~ age, data = dt.train, 
  control = rpart.control(minbucket = .01 * nrow(dt.train)))
```

#### Binning for Logistic Model
The usage of `bin.rpart` is very similar `rpart`, except that the *control* argument in `rpart` is named as *rcontrol* in `bin.rpart`. The following code generates the  cut points and bins for **age** and **platelet**. The cut points (cut.points) and bins are saved in a list as the function outputs. 
```{r}
lg.bin.age <- bin.rpart(formula = status ~ age, data = dt.train, 
  rcontrol = rpart.control(minbucket = .01 * nrow(dt.train)))

str(lg.bin.age)

lg.bin.platelet <- bin.rpart(formula = status ~ platelet, data = dt.train, 
  rcontrol = rpart.control(minbucket = .01 * nrow(dt.train)))
```


#### Binning for Survival Model
Compared to other packages (such as `smbinning` and `woe`) that only provides binning for logistic model, `bin.rpart` can provide the optimal binning for all models that can be passed to the `rpart` function. For example, the `bin.rpart` function can generate the optimal cut points of **age** in a survival model, if we change the dependent variable to a survival object (`Surv(time, status)`) in the formula. 
```{r}
surv.bin.age <- bin.rpart(formula = Surv(time, status) ~ age, data = dt.train,
  rcontrol = rpart.control(minbucket = .05 * nrow(dt.train)))  ## cp = 0.01
```


#### Search for Appropriate Number of Cut Points
By default, the cp (complexity parameter used to control `rpart`. The detail of the *cp* argument can be checked with `?rpart.control`) is set as 0.01, which is a little conservative for the survival model. Thus the number of cut points is usually small for the survival model, if we use the default *cp* value. We can reduce the *cp* value (e.g., 0.001) to get more cut points. We may be able to achieve an appropriate number of cut points by changing the *cp* argument repeatedly. 

In stead of changing the *cp* argument manually, the *n.group* (number of acceptable binning groups, can be a single number or a range) argument can help to find the appropriate number of cut points automatically. For example, if we set the acceptable *n.group* as 3:7, the `bin.rpart` function will try different *cp*, until the number of binning groups is within 3 to 7 (or the number of cut points within 2:6). 
```{r}
surv.bin.age2 <- bin.rpart(formula = Surv(time, status) ~ age, data = dt.train,
  rcontrol = rpart.control(minbucket = .05 * nrow(dt.train)), n.group = 3:7)
```


#### Replace Numerical Varialbes with Bins
After binning, we can replace the original numerical values with the corresponding bins saved in the outputs from `bin.rpart`.
```{r}
# We don't need the time column anmore, delete it in both dt.train and dt.test
dt.train <- dplyr::select(dt.train, -time)  
dt.test <- dplyr::select(dt.test, -time)
head(dt.train)
dt.train$age <- lg.bin.age$bins
dt.train$platelet <- lg.bin.platelet$bins
head(dt.train)
```
After replacing the numerical variables with bins, the numerical varialbes are converted to categorical variables automatically. 


## Level Statistics (Frequence, Rate, WOE, and IV): `level.stat`
For all of the categorical varialbes, it is useful to calculate some statistics of different levels. The `level.stat` function is designed for this purpose. 
```{r}
col.x <- c('age', 'gender', 'platelet', 'stage')
stat.train <- level.stat(dt.train, x = col.x, y = 'status')
head(stat.train)
```


## Visualize Level Statistics: `ggstat`

#### Plot with Default Arguments
```{r}
ggstat(data = stat.train, var = 'Variable.IV', x = 'Group', y = 'Rate.1',
  y.label = 'Perc.1', y.min.0 = FALSE, y.title = NULL, bar.width = 'Rate.group',
  bar.width.label = 'Perc.group', n.col = NULL)
```

#### Constant Bar Width
```{r}
ggstat(stat.train, bar.width = NULL)
```

#### Plot WOE
```{r,fig.height=3}
ggstat(stat.train, y = 'WOE', y.label = 'WOE.round', bar.width = NULL, 
  bar.width.label = NULL, n.col = 4)
```


## Replace Bins with WOE: `replace.woe`

```{r}
replace.woe(data = dt.train, level.stat.output = stat.train, replace = FALSE) %>%
  head

dt.train <- replace.woe(data = dt.train, level.stat.output = stat.train, 
  replace = TRUE)
head(dt.train)
```


## Correlation between Independent Variables: `corrplot.beautify`

```{r}
cor.mat <- cor(dt.train[, col.x])
corrplot(cor.mat)
corrplot.beautify(cor.mat)
```


## Logistic Model

#### Full Model
```{r}
lg <- glm(status ~ ., dt.train, family=binomial(link='logit'))
summary(lg)
```

#### Stepwise Variable Selection
```{r}
lg.aic <- stepAIC(lg, k =  qchisq(0.05, 1, lower.tail=F))   # p to enter: 0.05
summary(lg.aic)

data.frame(vif(lg.aic))
```


## Prepare Test Data: `bin.custom & replace.woe`

#### Bin Test Data: `bin.custom`
```{r}
head(dt.test)
dt.test$age <-  bin.custom(dt.test$age, cut.p = lg.bin.age$cut.points)
dt.test$platelet <- bin.custom(dt.test$platelet, cut.p = lg.bin.platelet$cut.points)
head(dt.test)
```


#### Replace Binned Test Data with WOE: `replace.woe`
```{r}
dt.test <- replace.woe(dt.test, level.stat.output = stat.train, replace = TRUE)
head(dt.test)
```


## Model Performance: `perf.auc & perf.decile`

#### Check Performance Based on AUC: `perf.auc`
```{r}
perf.auc(model = lg.aic, dt.train, dt.test)
```


#### Check Performance Based on Decile Rate: `perf.decile`
```{r}
pred.test <- predict(lg.aic, newdata = dt.test, type = 'response')
perf.decile(actual = dt.test$status, pred = pred.test, add.legend = TRUE)
```



## Convert Coefficients to Rate: `coef2rate`

```{r}
summary(lg.aic)
pred.stat <- coef2rate(data = dt.test, model = lg.aic, 
  level.stat.output = stat.train, force.change = TRUE)
head(pred.stat)
# Compare The Actual & Prediction Rates
pred.stat[,c('Rate.1', 'Pred.Rate.1')]

ggstat(pred.stat, y = 'Pred.Rate.1')
```


## Reference:
* streamlineR package information: https://github.com/JianhuaHuang/streamlineR 
* Submit bug: http://github.com/JianhuaHuang/streamlineR/issues
